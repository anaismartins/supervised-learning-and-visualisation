---
title: "Practice"
author: "1760068 Ana Martins"
date: "2022-10-31"
output: html_document
---

## Q1

a) aesthetics: x is mapped to Hgt, y is mapped to Age, fill is mapped to Diff.
  geoms: geom_bar is used.
  scales: x and y binned.
  facets: none.
  
b) aesthetics: x is mapped to age, y is mapped to density and fill is mapped to area. the line color is black and alpha is set to lower than 1.
  geoms: geom_line().
  scales: continuous for both.
  facets: none.
  
## Q2

The data points seem randomly placed vertically. You can see the horizontal axis is the percentage, but there is no indicated vertical axis, so it makes no sense to see what happens in Asia for example, around the median, where the points are all together at some points and all apart at others. The labels seem random too, as only some countries are labeled, as if only ones that were labeled were the ones that had space for it. A better way to display this information would be, for example, to draw a histogram for each regional cluster and overlaying them with different colors and a low alpha. The current graoh does not work based on alignment, as the positioning does not make sense and on orientation, because it spreads out in a way that is hard to read and understand.

## Q3 **Energy prediction** You work for an energy company. From the cool smart energy meters in every customerâ€™s home you can collect features, measured on 15 minute intervals. Your goal is to predict whether the energy usage is over their prepayment or under (i.e. too much used or too little used. You have a hand-coded label for each row in the data. Your dataset has 5000 columns and 2000 rows

a) You want to perform logistic regression but it does not work when you have more columns than
rows. What would your strategy be? Be specific about the steps you would take!

From what I understand, our data looks something like this:

Step1   | Step2   | ... | Step5000
Values1 | Values2 | ... | Values5000

With ValuesN being a vector with 2000 elements. Here, we should actually be looking at the data the other way round. The logs should be coming in to new rows, not columns, so we would have to transpose the current data.

b) From your logistic regression model, you obtain the confusion matrix below. What is the
accuracy? Is this high compared to the baseline accuracy?

Accuracy is (TP + TN) / (P + N) = (1752 + 74) / (1752 + 26 + 148 + 74) = 91.3%

Over has more so using baseline we would have

          true
predicted over under
over      1900 100
under     0    0

So the baseline accuracy is (1900) / (1900 + 100) = 95%

The baseline accuracy is higher than the accuracy from our model.

```{r}
library(DAAG)
library(glmnet)
library(tidyverse)
set.seed(45)
```

## Q5

a) In the data set head.injury (from package DAAG), obtain a logistic regression model relating
clinically.important.brain.injury to all the other variables.

```{r}
lr_mod <- glm(formula = clinically.important.brain.injury ~ ., data = headInjury, family = binomial(link = "logit"))
```

b) Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a
risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT and indicate three
different scenarios that would satisfy the threshold. - NOT FINISHED

```{r}
threshold = 0.025
lr_pred <- predict(lr_mod)

data <-
  headInjury %>% 
  mutate(prediction = lr_pred) %>% 
  mutate(ct = ifelse(lr_pred < log(threshold/(1-threshold)), 0, 1))

table(true = data$ct, pred = data$clinically.important.brain.injury)
```

Three example scenarios that satisfy the threshold:
- The patient does not have any of the symptoms except having the Glasgow Coma Scale at 15 after 2 hours.
- The patient does not have any of the symptoms exccept being over 65 and being high risk.
- The patient does not have any of the symptoms excepthaving had amnesia before and loss of counsciousness.

## Q6 LASSO

In this question, the goal is to predict y from x.

a) Load the workspace data.Rdata and show an informative plot of the y and x space.

```{r}
load("data.Rdata")
```

```{r}
examdata <-
  as_tibble(examdata) 

newdata <-
  examdata %>%
  mutate(
    x1 = x[, 1],
    x2 = x[, 2],
    x3 = x[, 3],
    x4 = x[, 4],
    x5 = x[, 5],
    x6 = x[, 6],
    x7 = x[, 7],
    x8 = x[, 8],
    x9 = x[, 9],
    x10 = x[, 10],
    x11 = x[, 11],
    x12 = x[, 12],
    x13 = x[, 13],
    x14 = x[, 14],
    x15 = x[, 15],
    x16 = x[, 16],
    x17 = x[, 17],
    x18 = x[, 18],
    x19 = x[, 19],
    x20 = x[, 20],
    x21 = x[, 21],
    x22 = x[, 22],
    x23 = x[, 23],
    x24 = x[, 24],
    x25 = x[, 25],
    x26 = x[, 26],
    x27 = x[, 27],
    x28 = x[, 28],
    x29 = x[, 29],
    x30 = x[, 30]
  ) %>%
  select(-x) %>% 
  mutate(y = as.factor(y))
```

```{r}
newdata %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = y, fill = y)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()
```

b) Run a 10-fold cross validated LASSO logistic (`family = "binomial"`) regression using the misclassification error as the criterion.

```{r}
model <- cv.glmnet(examdata$x, examdata$y, family = "binomial", type.measure = "class")
```

c) Make a prediction of the class labels at \lambda = 0.05, 0.01

```{r}
pred <- predict(model, newx = examdata$x, s = c(0.05, 0.01), type = "class")
```

d) create a plot that shows the values of \lambda. What is the optimal value and why?

```{r}
plot(model)
model
```

The optimal value for \lambda is the 1se, \lambda = 0.04945, as it is a simpler model that the one used for lambda min and with similar predictive power.